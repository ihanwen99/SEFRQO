{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA RTX A6000. Num GPUs = 1. Max memory: 47.413 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading 3B-JOB-ckpts/ckpt-4296 with actual GPU utilization = 79.49%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 47.41 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2500. Num Sequences = 320.\n",
      "Unsloth: vLLM's KV Cache can use up to 31.52 GB. Also swap space = 6 GB.\n",
      "INFO 04-05 20:22:35 config.py:549] This model supports multiple tasks: {'embed', 'classify', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'fp4', 'bnb_4bit_use_double_quant': False, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': [], 'llm_int8_threshold': 6.0}\n",
      "INFO 04-05 20:22:35 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='3B-JOB-ckpts/ckpt-4296', speculative_config=None, tokenizer='3B-JOB-ckpts/ckpt-4296', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2500, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=3B-JOB-ckpts/ckpt-4296, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":320}, use_cached_outputs=False, \n",
      "INFO 04-05 20:22:35 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 04-05 20:22:36 model_runner.py:1110] Starting to load model 3B-JOB-ckpts/ckpt-4296...\n",
      "INFO 04-05 20:22:36 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W405 20:22:36.316202794 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.37it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.37it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-05 20:22:37 model_runner.py:1115] Loading model weights took 2.1360 GB\n",
      "INFO 04-05 20:22:37 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 04-05 20:22:38 worker.py:267] Memory profiling takes 0.84 seconds\n",
      "INFO 04-05 20:22:38 worker.py:267] the current vLLM instance can use total_gpu_memory (47.41GiB) x gpu_memory_utilization (0.79) = 37.69GiB\n",
      "INFO 04-05 20:22:38 worker.py:267] model weights take 2.14GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.49GiB; the rest of the memory reserved for KV Cache is 34.00GiB.\n",
      "INFO 04-05 20:22:38 executor_base.py:111] # cuda blocks: 19896, # CPU blocks: 3510\n",
      "INFO 04-05 20:22:38 executor_base.py:116] Maximum concurrency for 2500 tokens per request: 127.33x\n",
      "INFO 04-05 20:22:40 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 43/43 [00:25<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-05 20:23:05 model_runner.py:1562] Graph capturing finished in 25 secs, took 0.76 GiB\n",
      "INFO 04-05 20:23:05 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 28.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.3.19 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072, padding_idx=128009)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "base_model_name = \"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\"\n",
    "local_model_name = \"3B-GRPO/3B-GRPO-checkpoint-1000\"\n",
    "model_name = local_model_name\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_name)\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=local_model_name,\n",
    "    max_seq_length=2500,\n",
    "    load_in_4bit=True,\n",
    "    fast_inference=True,\n",
    "    gpu_memory_utilization=0.8,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ 3. 推理函数\n",
    "\n",
    "gbnf_grammar = \"\"\"\n",
    "<sql_hint> ::= \"/*+ \" <hint_list> \" */\"\n",
    "<hint_list> ::= <hint> (\" \" <hint>)*\n",
    "\n",
    "<hint> ::= <join_hint> | <scan_hint> | <leading_hint>\n",
    "\n",
    "<join_hint> ::= \"MergeJoin(\" <table_list> \")\" \n",
    "              | \"NestLoop(\" <table_list> \")\" \n",
    "              | \"HashJoin(\" <table_list> \")\"\n",
    "\n",
    "<scan_hint> ::= \"SeqScan(\" <table_name> \")\" \n",
    "              | \"IndexScan(\" <table_name> \")\"\n",
    "\n",
    "<leading_hint> ::= \"Leading(\" <nested_tables> \")\"\n",
    "\n",
    "<table_list> ::= <table_name> (\" \" <table_name>)*\n",
    "<table_name> ::= [a-zA-Z0-9_]+\n",
    "<nested_tables> ::= \"(\" <table_or_group> \")\"\n",
    "<table_or_group> ::= <table_name> \n",
    "                   | \"(\" <table_or_group> \" \" <table_or_group> \")\"\n",
    "\"\"\"\n",
    "\n",
    "def run_inference(prompt: str, max_new_tokens: int = 64):\n",
    "    \"\"\" 使用微调后的模型进行推理，返回生成的文本 \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # ✅ 确保 tokenizer 返回字典格式\n",
    "    inputs = tokenizer(\n",
    "        tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        ),\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ✅ 4. 运行推理\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,          # 控制生成的随机性\"\n",
    "        top_p=0.9,               # nucleus sampling\n",
    "        repetition_penalty=1.1,  # 减少重复生成\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    # ✅ 5. 解码输出\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 6. 测试推理\n",
    "test_prompt = \"解释深度学习中的卷积神经网络，并给出一个示例。\"\n",
    "result = run_inference(test_prompt, max_new_tokens=1280)\n",
    "print(f\"\\n=== 🚀 Inference Output ===\\n{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"\n",
    "# You are a Database Assistant which help the internal decide the join order. Based on the database schema are some cardinality of the filters, you will construct a query plan tree for the internal to execute. \n",
    "Please Respond in the following format:\n",
    "<think>...</think>and directly give the final answer within the example format <answer>...</answer>\n",
    "\n",
    "# You can do these actions:\n",
    "Check whether there are any remaining subtrees and tables haven't been joined.\n",
    "Decide whether to divide the candidate table set into two subset, and tackle each of them recursively(this is because the action space is a bushy tree)\n",
    "Pick one table from the candidate set(or subset), assign an \"IndexScan\", \"IndexOnlyScan\" or \"SeqScan\" label for it.\n",
    "Join one table(or an existing subtree, with a label) to an existing subtree, or another table, and assign a \"HashJoin\", \"NestLoop\" or \"MergeJoin\" label for that node.\n",
    "\n",
    "# You have these restrictions on your actions:\n",
    "Each table (may with a unique alias or just the table name) should be picked and picked only once.\n",
    "The table must be picked before Join.\n",
    "Each subtree/table could be joined only once, and after the join operation a new subtree appears, the old two are combined.\n",
    "Count on the newest subtree appear in each join step in the check operation.\n",
    "After join operation, you must do the check operation, to check if there exists any subtrees and tables haven't been joined.\n",
    "After check operation, you must do the decide operation, to tackle the untouched tables and used tables separately. Unless all the tables and subtrees have been tackled.\n",
    "\n",
    "# Here is one example:\n",
    "## You have these tables:\n",
    "tag AS t1\n",
    "site AS s1\n",
    "question AS q1\n",
    "tag_question AS tq1\n",
    "so_user AS u1\n",
    "tag AS t2\n",
    "site AS s2\n",
    "question AS q2\n",
    "tag_question AS tq2\n",
    "so_user AS u2\n",
    "account AS account\n",
    "\n",
    "## table cardinality = {\n",
    "    'tag': 186770,\n",
    "    'site': 173,\n",
    "    'question': 12663270,\n",
    "    'tag_question': 36885060,\n",
    "    'so_user': 21096884,\n",
    "    'account': 13863416,\n",
    "}\n",
    "\n",
    "## filter cardinality = {\n",
    "    ('site AS s1', '(s1.site_name='stats')'): 1,\n",
    "    ('tag AS t1', '(t1.name  = 'machine-learning')'): 3,\n",
    "    ('site AS s2', '(s2.site_name='stackoverflow')'): 1,\n",
    "    ('tag AS t2', '(t2.name  = 'heroku')'): 3,\n",
    "}\n",
    "\n",
    "Analyzes the given tables and filter conditions to determine the best join order.\n",
    "<answer>/*+ NestedLoop(t1 s1 tq1 q1 u1 t2 s2 tq2 q2 u2 account)\n",
    " HashJoin(t1 s1 tq1 q1 u1 t2 s2 tq2 q2 u2)\n",
    " NestedLoop(t2 s2 tq2 q2 u2)\n",
    " NestedLoop(t2 s2 tq2 q2)\n",
    " NestedLoop(t2 s2 tq2)\n",
    " MergeJoin(t2 s2)\n",
    " NestedLoop(t1 s1 tq1 q1 u1)\n",
    " NestedLoop(t1 s1 tq1 q1)\n",
    " NestedLoop(t1 s1 tq1)\n",
    " MergeJoin(t1 s1)\n",
    " SeqScan(t1)\n",
    " SeqScan(s1)\n",
    " IndexOnlyScan(tq1)\n",
    " IndexScan(q1)\n",
    " IndexScan(u1)\n",
    " SeqScan(t2)\n",
    " SeqScan(s2)\n",
    " IndexOnlyScan(tq2)\n",
    " IndexScan(q2)\n",
    " IndexScan(u2)\n",
    " IndexScan(account)\n",
    " Leading(((((((t1 s1) tq1) q1) u1) ((((t2 s2) tq2) q2) u2)) account)) */</answer>\n",
    "\n",
    "## Here is the question you need to solve. Please provide the hint as the example shows.\n",
    "# You have these tables:\n",
    "title AS t\n",
    "kind_type AS kt\n",
    "info_type AS it1\n",
    "movie_info AS mi1\n",
    "movie_info AS mi2\n",
    "info_type AS it2\n",
    "cast_info AS ci\n",
    "role_type AS rt\n",
    "name AS n\n",
    "movie_keyword AS mk\n",
    "keyword AS k\n",
    "\n",
    "# table cardinality = {\n",
    "    'title': 2528553,\n",
    "    'kind_type': 7,\n",
    "    'info_type': 113,\n",
    "    'movie_info': 14814177,\n",
    "    'cast_info': 36244344,\n",
    "    'role_type': 12,\n",
    "    'name': 4167491,\n",
    "    'movie_keyword': 4523930,\n",
    "    'keyword': 134170,\n",
    "}\n",
    "\n",
    "# filter cardinality = {\n",
    "    ('info_type AS it1', '((it1.id in ('7')))'): 1,\n",
    "    ('info_type AS it2', '((it2.id in ('8')))'): 1,\n",
    "    ('movie_info AS mi1', '((mi1.info in ('MET:600 m','OFM:35 mm','PCS:Spherical','PFM:35 mm','RAT:1.37 : 1')))'): 706025,\n",
    "    ('movie_info AS mi2', '((mi2.info in ('France','Germany','Japan','Mexico','Portugal','Spain','UK','USA')))'): 929343,\n",
    "    ('kind_type AS kt', '((kt.kind in ('tv series','video game','video movie')))'): 3,\n",
    "    ('role_type AS rt', '((rt.role in ('producer')))'): 1,\n",
    "    ('name AS n', '((n.gender in ('m') OR n.gender IS NULL))'): 2591471,\n",
    "    ('title AS t', '((t.production_year <= 2000)) AND ((t.production_year >= 1925))'): 970437,\n",
    "}\n",
    "\"\"\"\n",
    "result = run_inference(test_prompt, max_new_tokens=5000)\n",
    "print(f\"\\n=== 🚀 Inference Output ===\\n{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbnf_grammar = \"\"\"\n",
    "<sql_hint> ::= \"/*+ \" <hint_list> \" */\"\n",
    "<hint_list> ::= <hint> (\" \" <hint>)*\n",
    "<hint> ::= <join_hint> | <scan_hint> | <leading_hint>\n",
    "<join_hint> ::= \"MergeJoin(\" <table_list> \")\" \n",
    "              | \"NestLoop(\" <table_list> \")\" \n",
    "              | \"HashJoin(\" <table_list> \")\"\n",
    "<scan_hint> ::= \"SeqScan(\" <table_name> \")\" \n",
    "              | \"IndexScan(\" <table_name> \")\"\n",
    "<leading_hint> ::= \"Leading(\" <nested_tables> \")\"\n",
    "<table_list> ::= <table_name> (\" \" <table_name>)*\n",
    "<table_name> ::= [a-zA-Z0-9_]+\n",
    "<nested_tables> ::= \"(\" <table_or_group> \")\"\n",
    "<table_or_group> ::= <table_name> \n",
    "                   | \"(\" <table_or_group> \" \" <table_or_group> \")\"\n",
    "\"\"\"\n",
    "\n",
    "# 构造 prompt，要求输出必须严格按照 JSON 格式生成：\n",
    "# 字段 \"grammar\" 对应上面的语法文本，\n",
    "# 字段 \"example\" 是一个符合该语法的 SQL Hint 示例\n",
    "prompt = f\"\"\"请根据下面的 GBNF SQL Hint 语法生成输出。\n",
    "要求输出为 JSON 格式，并且必须包含两个字段：\n",
    "  1. \"grammar\"：值为下面提供的语法；\n",
    "  2. \"example\"：值为一个符合该语法的 SQL Hint 示例，例如：/*+ SeqScan(table1) */\n",
    "字段顺序必须固定为 \"grammar\" 在前，\"example\" 在后，且输出中不能包含其他内容。\n",
    "\n",
    "GBNF 语法:\n",
    "{gbnf_grammar}\n",
    "\"\"\"\n",
    "result = run_inference(prompt, max_new_tokens=5000)\n",
    "print(f\"\\n=== 🚀 Inference Output ===\\n{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
